{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Adjacency Matrix:\n",
      "tensor([[0., 1., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_nodes = 10\n",
    "\n",
    "sparsity = 0.3\n",
    "\n",
    "adjacency_matrix = torch.rand(num_nodes, num_nodes)\n",
    "adjacency_matrix = (adjacency_matrix < sparsity).float()\n",
    "\n",
    "\n",
    "adjacency_matrix = adjacency_matrix * (1 - torch.eye(num_nodes))\n",
    "\n",
    "print(\"Random Adjacency Matrix:\")\n",
    "print(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 5, 1, 8, 6, 9, 5, 5, 1, 1],\n",
      "         [9, 3, 7, 1, 4, 2, 2, 6, 6, 9],\n",
      "         [8, 3, 2, 5, 5, 3, 5, 5, 9, 8],\n",
      "         [9, 5, 9, 6, 8, 2, 6, 2, 7, 3],\n",
      "         [8, 7, 5, 1, 3, 3, 3, 4, 8, 7],\n",
      "         [5, 9, 3, 1, 1, 7, 5, 9, 2, 7],\n",
      "         [6, 5, 1, 8, 9, 2, 5, 8, 4, 1],\n",
      "         [7, 1, 6, 8, 6, 7, 9, 5, 4, 2],\n",
      "         [8, 5, 2, 8, 1, 9, 7, 9, 4, 1],\n",
      "         [7, 4, 2, 4, 4, 8, 6, 5, 3, 7]]])\n",
      "tensor([[6, 6, 3, 5, 8, 7, 6, 1, 2, 2],\n",
      "        [0, 6, 8, 2, 4, 5, 1, 3, 1, 3]])\n",
      "torch.Size([1, 10, 10, 5]) input\n",
      "torch.Size([10, 10, 5]) after input\n",
      "10 10 squezzed\n",
      "DataBatch(x=[100, 5], edge_index=[2, 100], batch=[100], ptr=[11])\n",
      "torch.Size([1, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "from model import CongestionLearnableEmbedding, CongestionWrapperEncoder\n",
    "import torch\n",
    "from parameters import CONGESTION_EMBEDDING_DIM\n",
    "\n",
    "cong = torch.randint(1, 10, size=(10, num_nodes)).unsqueeze(0)\n",
    "print(cong)\n",
    "\n",
    "edge_index = torch.randint(high=num_nodes-1, size=(2, num_nodes**2//10))\n",
    "print(edge_index)\n",
    "x = CongestionWrapperEncoder(CongestionLearnableEmbedding, num_nodes, CONGESTION_EMBEDDING_DIM, CONGESTION_EMBEDDING_DIM, 1, 0.1)\n",
    "# print(x(cong, edge_index))\n",
    "print(x(cong, edge_index).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 1., 0., 1., 1., 0.],\n",
      "         [0., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "         [1., 0., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 1., 0., 0., 1., 1., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 1., 0., 1., 1., 0.],\n",
      "         [0., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "         [1., 0., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 1., 0., 0., 1., 1., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 1., 0., 1., 1., 0.],\n",
      "         [0., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "         [1., 0., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 1., 0., 1., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 1., 0., 0., 1., 1., 1., 1., 0.]]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Static graphs not supported in 'GATConv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m num_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     19\u001b[0m gat_layer \u001b[38;5;241m=\u001b[39m GATConv(in_channels, out_channels, heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[0;32m---> 21\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgat_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of output_tensor:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Desktop/congestion/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/congestion/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/congestion/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/gat_conv.py:212\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# We first transform the input node features. If a tuple is passed, we\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# transform source and target node features via separate weights:\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, Tensor):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatic graphs not supported in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGATConv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m     x_src \u001b[38;5;241m=\u001b[39m x_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_src(x)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Tuple of source and target node features:\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Static graphs not supported in 'GATConv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "batch_size = 3\n",
    "num_nodes = 10\n",
    "num_features = 16\n",
    "\n",
    "x = torch.randn(batch_size, num_nodes, num_features)\n",
    "\n",
    "# edge_index = torch.randint(0, num_nodes, (2, num_nodes * 3))\n",
    "edge_index = torch.stack([adjacency_matrix for x in range(3)])\n",
    "print(edge_index)\n",
    "\n",
    "in_channels = num_features\n",
    "out_channels = 8\n",
    "num_heads = 2\n",
    "\n",
    "gat_layer = GATConv(in_channels, out_channels, heads=num_heads)\n",
    "\n",
    "output_tensor = gat_layer(x, edge_index)\n",
    "\n",
    "print(\"Shape of output_tensor:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.rand(3, 16)  \n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.gat_conv = GATConv(in_channels, out_channels, heads=heads, dropout=dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.gat_conv(data.x, data.edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_features = 16\n",
    "output_features = 8\n",
    "num_heads = 2\n",
    "\n",
    "graph_attention_layer = GraphAttentionLayer(input_features, output_features, heads=num_heads)\n",
    "\n",
    "output = graph_attention_layer(data)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Example: Batch of graphs with 3 graphs, each having 4 nodes and 16 features per node\n",
    "batch_size = 3\n",
    "num_nodes = 4\n",
    "num_features = 16\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.rand(batch_size, num_nodes, num_features)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.gat_conv = GATConv(in_channels, out_channels, heads=heads, dropout=dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.gat_conv(data.x, data.edge_index)\n",
    "        return x\n",
    "\n",
    "input_features = 16\n",
    "output_features = 8\n",
    "num_heads = 2\n",
    "\n",
    "graph_attention_layer = GraphAttentionLayer(input_features, output_features, heads=num_heads)\n",
    "\n",
    "# Forward pass through the GAT layer for the entire batch\n",
    "output = graph_attention_layer(data)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5, 3, 6, 1, 5, 7, 2, 8, 3, 6],\n",
      "         [8, 9, 9, 6, 7, 4, 3, 3, 6, 5],\n",
      "         [8, 6, 4, 4, 9, 2, 6, 5, 2, 6],\n",
      "         [3, 7, 4, 2, 5, 2, 9, 4, 9, 4],\n",
      "         [4, 5, 2, 2, 4, 4, 9, 8, 9, 7],\n",
      "         [6, 8, 7, 4, 5, 1, 3, 3, 6, 4],\n",
      "         [3, 6, 9, 4, 8, 6, 4, 4, 8, 8],\n",
      "         [8, 1, 3, 6, 7, 6, 7, 1, 7, 4],\n",
      "         [3, 3, 1, 6, 8, 6, 7, 3, 7, 1],\n",
      "         [4, 4, 5, 9, 9, 2, 2, 4, 6, 4]]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/isaigordeev/Desktop/congestion/models/test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isaigordeev/Desktop/congestion/models/test.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(high\u001b[39m=\u001b[39mnum_nodes\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, size\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, num_nodes\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaigordeev/Desktop/congestion/models/test.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# x = CongestionWrapperEncoder(CongestionLearnableEmbedding, num_nodes, CONGESTION_EMBEDDING_DIM, CONGESTION_EMBEDDING_DIM, 1, 0.1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaigordeev/Desktop/congestion/models/test.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# print(x(cong, edge_index))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaigordeev/Desktop/congestion/models/test.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# x(cong, edge_index).shape\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isaigordeev/Desktop/congestion/models/test.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m x \u001b[39m=\u001b[39m CongestionModel(CongestionLearnableEmbedding, CongestionWrapperEncoder, CongestionTransformerDecoder, \u001b[39mNone\u001b[39;49;00m, \u001b[39m10\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m0.1\u001b[39;49m)(cong, edge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaigordeev/Desktop/congestion/models/test.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m x\n",
      "File \u001b[0;32m~/Desktop/congestion/models/model.py:204\u001b[0m, in \u001b[0;36mCongestionModel.__init__\u001b[0;34m(self, congestion_embedding, congestion_wrapper_encoder, congestion_decoder, num_nodes, embedding_dim, in_channels_wrapper_encoder, out_channels_wrapper_encoder, num_heads_wrapper_encoder, dropout_wrapper_encoder, num_transformer_layers, mlp_size, num_heads, attn_dropout)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m              congestion_embedding: CongestionLearnableEmbedding,\n\u001b[1;32m    188\u001b[0m              congestion_wrapper_encoder: CongestionWrapperEncoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m              attn_dropout: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m    201\u001b[0m              ):\n\u001b[1;32m    202\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcongestion_wrapper_encoder \u001b[39m=\u001b[39m congestion_wrapper_encoder(congestion_embedding,\n\u001b[1;32m    205\u001b[0m                                                                  num_nodes,\n\u001b[1;32m    206\u001b[0m                                                                  in_channels_wrapper_encoder,\n\u001b[1;32m    207\u001b[0m                                                                  out_channels_wrapper_encoder,\n\u001b[1;32m    208\u001b[0m                                                                  num_heads_wrapper_encoder,\n\u001b[1;32m    209\u001b[0m                                                                  dropout_wrapper_encoder)\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcongestion_decoder \u001b[39m=\u001b[39m congestion_decoder(num_nodes,\n\u001b[1;32m    212\u001b[0m                                                  num_transformer_layers,\n\u001b[1;32m    213\u001b[0m                                                  embedding_dim,\n\u001b[1;32m    214\u001b[0m                                                  mlp_size,\n\u001b[1;32m    215\u001b[0m                                                  num_heads,\n\u001b[1;32m    216\u001b[0m                                                  attn_dropout)\n",
      "File \u001b[0;32m~/Desktop/congestion/models/model.py:36\u001b[0m, in \u001b[0;36mCongestionWrapperEncoder.__init__\u001b[0;34m(self, embeddings, node_number, in_channels, out_channels, heads, dropout)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     27\u001b[0m             embeddings: CongestionLearnableEmbedding,\n\u001b[1;32m     28\u001b[0m             node_number\u001b[39m=\u001b[39mDEFAULT_TERMINAL_NUMBER,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m             heads\u001b[39m=\u001b[39mATTENTION_HEADS,\n\u001b[1;32m     32\u001b[0m             dropout\u001b[39m=\u001b[39mDROPOUT):\n\u001b[1;32m     34\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcongestion_embeddings \u001b[39m=\u001b[39m embeddings(node_number, in_channels)\n\u001b[1;32m     38\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_number \u001b[39m=\u001b[39m node_number\n\u001b[1;32m     39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_channels \u001b[39m=\u001b[39m in_channels\n",
      "File \u001b[0;32m~/Desktop/congestion/models/model.py:18\u001b[0m, in \u001b[0;36mCongestionLearnableEmbedding.__init__\u001b[0;34m(self, num_tokens, embedding_dim)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, num_tokens\u001b[39m=\u001b[39mCONGESTION_EMBEDDING_CARDINAL, embedding_dim\u001b[39m=\u001b[39mCONGESTION_EMBEDDING_DIM):\n\u001b[1;32m     17\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(num_tokens, embedding_dim)\n",
      "File \u001b[0;32m~/Desktop/congestion/venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:142\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_grad_by_freq \u001b[39m=\u001b[39m scale_grad_by_freq\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs),\n\u001b[1;32m    143\u001b[0m                             requires_grad\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m _freeze)\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_parameters()\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "from model import CongestionLearnableEmbedding, CongestionWrapperEncoder, CongestionModel, CongestionTransformerDecoder\n",
    "import torch\n",
    "from parameters import CONGESTION_EMBEDDING_DIM\n",
    "num_nodes = 10\n",
    "cong = torch.randint(1, 10, size=(10, num_nodes)).unsqueeze(0)\n",
    "print(cong)\n",
    "\n",
    "edge_index = torch.randint(high=num_nodes-1, size=(2, num_nodes**2//10))\n",
    "\n",
    "# x = CongestionWrapperEncoder(CongestionLearnableEmbedding, num_nodes, CONGESTION_EMBEDDING_DIM, CONGESTION_EMBEDDING_DIM, 1, 0.1)\n",
    "# print(x(cong, edge_index))\n",
    "# x(cong, edge_index).shape\n",
    "\n",
    "x = CongestionModel(CongestionLearnableEmbedding, CongestionWrapperEncoder, CongestionTransformerDecoder, None, 10, 5, 5, 1, 0.1)(cong, edge_index)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
